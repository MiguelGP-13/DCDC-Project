{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c233c499",
   "metadata": {},
   "source": [
    "## Limpieza de textos\n",
    "\n",
    "Vamos a eliminar genero y carácteres redundantes de la columna descripcion, además de preprocesar y tokenizar el texto para facilitar la lectura al modelo\n",
    "\n",
    "Cambios: \n",
    "\n",
    " Eliminar URL (realizando antes)<br>\n",
    " Eliminar números (realizando antes)<br>\n",
    " Eliminar correos (realizando antes) <br> <br>\n",
    " Traducción de textos en otras lenguas al castellano. <br>\n",
    " Convertir a minúsculas<br>\n",
    " Eliminar signos de puntuación<br>\n",
    " Lematizar texto<br>\n",
    " Detectar y eliminar patrones tipo /a, /as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b206ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from googletrans import Translator\n",
    "\n",
    "input_file = 'ofertasAnonimizado_unique_ts.csv'\n",
    "output_file = 'ofertasAnonimizado_unique_ts_traducido.csv'\n",
    "provincias_catalanas = {'Barcelona', 'Girona', 'Tarragona', 'Lleida'}\n",
    "\n",
    "translator = Translator()\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as fin, open(output_file, 'w', encoding='utf-8', newline='') as fout:\n",
    "    reader = csv.DictReader(fin, delimiter=';')\n",
    "    fieldnames = reader.fieldnames\n",
    "    writer = csv.DictWriter(fout, fieldnames=fieldnames, delimiter=';')\n",
    "    writer.writeheader()\n",
    "    for row in reader:\n",
    "        region = row.get('region', '')\n",
    "        if region in provincias_catalanas:\n",
    "            desc = row.get('descripcion', '')\n",
    "            # Detectar si hay catalán (heurística simple: palabras típicas)\n",
    "            if any(pal in desc.lower() for pal in ['amb', 'per', 'dilluns', 'divendres', 'dimecres', 'dijous', 'dissabte', 'diumenge', 'treballar', 'ofereix', 'incorporació', 'contracte', 'empresa', 'persones', 'serveis', 'activitat', 'atenció', 'domiciliària', 'grans', 'català', 'castellà', 'client', 'oferta', 'inscripció', 'referència', 'convocatòria', 'col·lectiu', 'conveni', 'salarial', 'torn', 'matí', 'tarda', 'horari', 'dilluns', 'divendres', 'dimecres', 'dijous', 'dissabte', 'diumenge']):\n",
    "                try:\n",
    "                    translated = translator.translate(desc, src='ca', dest='es').text\n",
    "                    row['descripcion'] = translated\n",
    "                except Exception:\n",
    "                    pass  # Si falla la traducción, deja el texto original\n",
    "        writer.writerow(row)\n",
    "print('Traducción completada')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ccd2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae53a387",
   "metadata": {},
   "source": [
    "## sentiment analysis\n",
    "Sobre la columna descripcion vamos a usar distintos modelos que luego haremos usaremos para un voting clasifier.\n",
    "\n",
    "Nuestros modelos: <br>\n",
    "Vader de nltk, bastante simple y rápido para procesar texto<br>\n",
    "TextBlob, también un modelo sencillo para procesar sentimientos<br>\n",
    "BERT, modelo basado en transformers más complejo, debería dar un desempeño mejor, por eso consideramos más importancia a este modelo en las votaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cde89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\alvar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('vader_lexicon') # descargar el modelo para usar el sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb8df648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\alvar\\.cache\\huggingface\\hub\\models--nlptown--bert-base-multilingual-uncased-sentiment. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m sia = SentimentIntensityAnalyzer()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m bert_pipeline = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentiment-analysis\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnlptown/bert-base-multilingual-uncased-sentiment\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# scores por clase\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvader_scores\u001b[39m(text):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\pipelines\\base.py:268\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    266\u001b[39m         classes.append(_class)\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m look_tf:\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     _class = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtransformers_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTF\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43marchitecture\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    270\u001b[39m         classes.append(_class)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1026\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\models\\bert\\modeling_tf_bert.py:27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_outputs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     29\u001b[39m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[32m     30\u001b[39m     TFBaseModelOutputWithPoolingAndCrossAttentions,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     TFTokenClassifierOutput,\n\u001b[32m     38\u001b[39m )\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     40\u001b[39m     TFCausalLanguageModelingLoss,\n\u001b[32m     41\u001b[39m     TFMaskedLanguageModelingLoss,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     unpack_inputs,\n\u001b[32m     53\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\alvar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "bert_pipeline = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# scores por clase\n",
    "def vader_scores(text):\n",
    "    scores = sia.polarity_scores(text)\n",
    "    return {\n",
    "        \"neg\": scores[\"neg\"],\n",
    "        \"neu\": scores[\"neu\"],\n",
    "        \"pos\": scores[\"pos\"]\n",
    "    }\n",
    "\n",
    "def textblob_scores(text):\n",
    "    polarity = TextBlob(text).sentiment.polarity\n",
    "    if polarity > 0:\n",
    "        return {\"neg\": 0.0, \"neu\": 1 - polarity, \"pos\": polarity}\n",
    "    elif polarity < 0:\n",
    "        return {\"neg\": abs(polarity), \"neu\": 1 - abs(polarity), \"pos\": 0.0}\n",
    "    else:\n",
    "        return {\"neg\": 0.0, \"neu\": 1.0, \"pos\": 0.0}\n",
    "\n",
    "def bert_scores(text):\n",
    "    result = bert_pipeline(text)[0]['label']\n",
    "    # convertir estrellas a valores aproximados\n",
    "    if \"1\" in result:\n",
    "        return {\"neg\": 0.9, \"neu\": 0.1, \"pos\": 0.0}\n",
    "    elif \"2\" in result:\n",
    "        return {\"neg\": 0.7, \"neu\": 0.3, \"pos\": 0.0}\n",
    "    elif \"3\" in result:\n",
    "        return {\"neg\": 0.2, \"neu\": 0.6, \"pos\": 0.2}\n",
    "    elif \"4\" in result:\n",
    "        return {\"neg\": 0.0, \"neu\": 0.2, \"pos\": 0.8}\n",
    "    elif \"5\" in result:\n",
    "        return {\"neg\": 0.0, \"neu\": 0.1, \"pos\": 0.9}\n",
    "\n",
    "\n",
    "# Ensemble ponderado\n",
    "def weighted_sentiment(text, weights=(0.3, 0.3, 0.4)):\n",
    "    vader_w, blob_w, bert_w = weights\n",
    "    \n",
    "    v = vader_scores(text)\n",
    "    t = textblob_scores(text)\n",
    "    b = bert_scores(text)\n",
    "    \n",
    "    combined = {\n",
    "        \"neg\": vader_w*v[\"neg\"] + blob_w*t[\"neg\"] + bert_w*b[\"neg\"],\n",
    "        \"neu\": vader_w*v[\"neu\"] + blob_w*t[\"neu\"] + bert_w*b[\"neu\"],\n",
    "        \"pos\": vader_w*v[\"pos\"] + blob_w*t[\"pos\"] + bert_w*b[\"pos\"]\n",
    "    }\n",
    "    \n",
    "    # clasificar\n",
    "    final_label = max(combined, key=combined.get)\n",
    "    label_map = {\"neg\": -1, \"neu\": 0, \"pos\": 1}\n",
    "    return label_map[final_label]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "811b8d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ocupacion</th>\n",
       "      <th>descripcion</th>\n",
       "      <th>provincia</th>\n",
       "      <th>tipo_contrato</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://europa.eu/eures/portal/jv-se/jv-detail...</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>AGENTE COMERCIAL DE SEGUROS (REF.: 6891)</td>\n",
       "      <td>corredor de seguros/corredora de seguros</td>\n",
       "      <td>TAREAS:Prospección de nuevos asegurados/as.Pla...</td>\n",
       "      <td>Asturias</td>\n",
       "      <td>Contrato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://europa.eu/eures/portal/jv-se/jv-detail...</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>PERSONAL CONDUCCIÓN DE CAMIONES RÍGIDOS Y GÓND...</td>\n",
       "      <td>Conductor de vehículo de carga/conductora de v...</td>\n",
       "      <td>DESCRIPCIÓN: Se necesita cubrir cuatro puestos...</td>\n",
       "      <td>Huesca</td>\n",
       "      <td>Contrato</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://europa.eu/eures/portal/jv-se/jv-detail...</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>EDUCADORES SOCIALES</td>\n",
       "      <td>Trabajador social/trabajadora social</td>\n",
       "      <td>EDUCADOR/A SOCIAL PARA HOGAR EN ARINAGA. FINES...</td>\n",
       "      <td>Las Palmas</td>\n",
       "      <td>Determinado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://europa.eu/eures/portal/jv-se/jv-detail...</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>PIZZERO/A (REF. 042025002051)</td>\n",
       "      <td>Pizzero/pizzera</td>\n",
       "      <td>FUNCIONES: Elaboración de pizzas REQUISITOS: 2...</td>\n",
       "      <td>Islas Baleares</td>\n",
       "      <td>Determinado</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://europa.eu/eures/portal/jv-se/jv-detail...</td>\n",
       "      <td>10/10/2025</td>\n",
       "      <td>INTÉRPRETES DE LA LENGUA DE SIGNOS</td>\n",
       "      <td>intérprete de lengua de signos</td>\n",
       "      <td>INTÉRPRETE DE LENGUA DE SIGNOS PARA PUESTOS EN...</td>\n",
       "      <td>Santa Cruz de Tenerife</td>\n",
       "      <td>Determinado</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  id   timestamp  \\\n",
       "0  https://europa.eu/eures/portal/jv-se/jv-detail...  10/10/2025   \n",
       "1  https://europa.eu/eures/portal/jv-se/jv-detail...  10/10/2025   \n",
       "2  https://europa.eu/eures/portal/jv-se/jv-detail...  10/10/2025   \n",
       "3  https://europa.eu/eures/portal/jv-se/jv-detail...  10/10/2025   \n",
       "4  https://europa.eu/eures/portal/jv-se/jv-detail...  10/10/2025   \n",
       "\n",
       "                                              titulo  \\\n",
       "0           AGENTE COMERCIAL DE SEGUROS (REF.: 6891)   \n",
       "1  PERSONAL CONDUCCIÓN DE CAMIONES RÍGIDOS Y GÓND...   \n",
       "2                                EDUCADORES SOCIALES   \n",
       "3                      PIZZERO/A (REF. 042025002051)   \n",
       "4                 INTÉRPRETES DE LA LENGUA DE SIGNOS   \n",
       "\n",
       "                                           ocupacion  \\\n",
       "0           corredor de seguros/corredora de seguros   \n",
       "1  Conductor de vehículo de carga/conductora de v...   \n",
       "2               Trabajador social/trabajadora social   \n",
       "3                                    Pizzero/pizzera   \n",
       "4                     intérprete de lengua de signos   \n",
       "\n",
       "                                         descripcion               provincia  \\\n",
       "0  TAREAS:Prospección de nuevos asegurados/as.Pla...                Asturias   \n",
       "1  DESCRIPCIÓN: Se necesita cubrir cuatro puestos...                  Huesca   \n",
       "2  EDUCADOR/A SOCIAL PARA HOGAR EN ARINAGA. FINES...              Las Palmas   \n",
       "3  FUNCIONES: Elaboración de pizzas REQUISITOS: 2...          Islas Baleares   \n",
       "4  INTÉRPRETE DE LENGUA DE SIGNOS PARA PUESTOS EN...  Santa Cruz de Tenerife   \n",
       "\n",
       "  tipo_contrato  \n",
       "0      Contrato  \n",
       "1      Contrato  \n",
       "2   Determinado  \n",
       "3   Determinado  \n",
       "4   Determinado  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a861a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"../empleos-espanoles-eures-2025.csv\", sep=\";\") # cambiar el dataset a su ultima version traducida y tokenizada\n",
    "df[\"sentimiento\"] = df[\"descripcion\"].apply(weighted_sentiment)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Mostrar resultado\n",
    "# -----------------------------\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
